# -*- coding: utf-8 -*-
"""
Created on Sun Dec 28 21:13:08 2025

@author: Ryan
"""


# =============================================================================
# MyClass()
# â†“
# PyObject_Call(MyClass)
# â†“
# PyType_Type.tp_call   â† å› ç‚º MyClass æ˜¯ type çš„ instance
# â†“
# type_call(type=MyClass)
# â†“
# MyClass.tp_new
# â†“
# MyClass.tp_init
# â†“
# instance
# =============================================================================
 
# =============================================================================
# ğŸ§© slot wrapper åˆ°åº•æ˜¯ä»€éº¼ï¼Ÿ
# ä¸æ˜¯ Python function
# ä¸æ˜¯ PyCFunction
# è€Œæ˜¯ã€ŒC å±¤ dispatcherã€
# 
# ç°¡åŒ–é•·ç›¸ï¼š
# 
# static PyObject *
# slot_tp_new(PyTypeObject *type, PyObject *args, PyObject *kwds)
# {
#     PyObject *meth = lookup("__new__", type);
#     return PyObject_Call(meth, args, kwds);
# }
# 
# 
# ğŸ‘‰ å®ƒçš„ä»»å‹™åªæœ‰ä¸€å€‹ï¼š
# 
# æŠŠ C slot å‘¼å«è½‰æˆ Python method å‘¼å«
# 
# ğŸ§© ç‚ºä»€éº¼ä¸èƒ½ç›´æ¥æŒ‡å‘ Python functionï¼Ÿ
# 
# å› ç‚ºï¼š
# 
# C slot ç°½åæ˜¯å›ºå®šçš„ï¼ˆABIï¼‰
# 
# Python function æ˜¯å‹•æ…‹ç‰©ä»¶
# 
# éœ€è¦ï¼š
# 
# argument unpack
# 
# descriptor ç¶å®š
# 
# MRO lookup
# 
# exception translation
# 
# ğŸ‘‰ slot wrapper æ˜¯å¿…è¦çš„ã€Œè½‰æ¥å±¤ã€
# =============================================================================

# =============================================================================
# ğŸ§  æœ€çµ‚ç¸½çµï¼ˆä½ ç¾åœ¨çš„ä½ç½®ï¼‰
# 
# ä½ ç¾åœ¨çš„ç†è§£å¯ä»¥æ¿ƒç¸®æˆé€™å¼µè¡¨ï¼š
# 
# æƒ…å¢ƒ	tp_repr æŒ‡å‘
# æ²’è¦†å¯« __repr__	C å¯¦ä½œï¼ˆbase.tp_reprï¼‰
# æœ‰è¦†å¯« __repr__	slot_tp_repr
# Python å‘¼å« obj.__repr__()	attribute lookup
# C / builtin å‘¼å« repr(obj)	tp_repr(obj)
# 
# ğŸ‘‰ slot wrapper ä¸æ˜¯å¤šæ­¤ä¸€èˆ‰
# ğŸ‘‰ å®ƒæ˜¯ç‚ºäº†è£œä¸Šã€ŒC slot å‘¼å«ä¸åš lookupã€é€™å€‹ç¼ºå£
# 
# =============================================================================

import numpy as np
from collections import deque, defaultdict

_GRADIENT_REGISTRY = {}

def register_gradient(op_name):
    def wrapper(func):
        _GRADIENT_REGISTRY[op_name] = func
        return func #é‡é»åŒ…è£å™¨é‚„æ˜¯å›å‚³åŸæœ¬è‡ªå·±çš„å‡½æ•¸
    return wrapper

class Tensor:
    def __init__(self,value, name=None):
        self.value = np.array(value, dtype=float)
        self.name = name
        self.id = id(self)
    def __repr__(self):
        return f"Tesnor(shape = {self.value.shape}, id={self.id})"
        
        
class Gradient_Tape:
    def __init__(self):
        self.ops = []
        self.active = False
    def __enter__(self):
        self.active = True
        global _CURRENT_TAPE
        _CURRENT_TAPE = self 
        return self #ç•¶ä½ å¸Œæœ›åœ¨ with èªå¥ä¸­å–å¾—ä¸¦æ“ä½œé€™å€‹ä¸Šä¸‹æ–‡ç®¡ç†å™¨å¯¦ä¾‹æ™‚ã€‚
    def __exit__(self, exc_type, exc_val, exc_tb):
        self.active = False
        global _CURRENT_TAPE
        _CURRENT_TAPE = None
        
    def record_op(self, op_name, inputs, output):
        self.ops.append({
            "op_name":op_name,
            "inputs":inputs,
            "output":output            
            })
    def gradient(self,target, sources):
        """
        å…¨éƒ¨é‹ç®—éƒ½åšå®Œ æ‰æœƒåœ¨å°loss fcnä½œå¾®åˆ† tape.gradient(Loss, [X, W, B]) 
        target : loss (Tensor) æœ€ä¸€é–‹å§‹é€²å…¥çµé»
        sources : list of params to update (list of Tensors)
        """
        # å“ªå€‹ output Tensor æ˜¯ç”±å“ªå€‹ op ç”¢ç”Ÿ
        producer_map = {}
        for op in self.ops:
            producer_map[op["output"].id] = op
            
        grad_counts = defaultdict(int)
        for op in self.ops:
            for x in op["inputs"]:
                grad_counts[x.id] += 1
        
        grads = defaultdict(int)
        grads[target.id] = np.ones_like(target.value)
        
        queue = deque([target])
        
        while queue:
            current_tensor = queue.popleft()
            current_grad = grads[current_tensor.id]
            
            if current_tensor.id not in producer_map:
                continue
            
            op_entry = producer_map[current_tensor.id]
            op_name = op_entry["op_name"]
            inputs = op_entry["inputs"]
            
            if op_name not in _GRADIENT_REGISTRY:
                raise ValueError(f"Op {op_name} æ²’æœ‰è¨»å†Šå¾®åˆ†å‡½æ•¸")
            grad_func = _GRADIENT_REGISTRY[op_name]  
            
            input_grads = grad_func(current_grad, inputs)
            
            if not isinstance(input_grads,(list, tuple)):
                input_grads = [input_grads]
            
            for x,g in zip(inputs, input_grads):
                # å»£æ’­ä¿®æ­£ (Broadcasting Fix)
                if g.shape != x.value.shape:
                    while g.ndim > x.value.ndim:
                        g = g.sum(axis=0)
                    for i, dim in enumerate(x.value.shape):
                        if dim == 1 and g.shape[i] != 1:
                            g = g.sum(axis=i, keepdims=True)
                            
                grads[x.id] += g
                grad_counts[x.id] -= 1
                
                if grad_counts[x.id] == 0:
                    queue.append(x)
        return [ grads[s.id] for s in sources]
            
def tf_matmul(a,b):
    val = a.value @ b.value
    out = Tensor(val)    
    
    if _CURRENT_TAPE:
        _CURRENT_TAPE.record_op("MatMul", [a,b], out)
    return out
        
def tf_add(a,b):
    val = a.value + b.value
    out = Tensor(val)    
    
    if _CURRENT_TAPE:
        _CURRENT_TAPE.record_op("Add", [a,b], out)
    return out

def tf_mul(a,b):
    val = a.value * b.value
    out = Tensor(val)    
    
    if _CURRENT_TAPE:
        _CURRENT_TAPE.record_op("Mul", [a,b], out)
    return out

def tf_sub(a,b):
    val = a.value - b.value
    out = Tensor(val)    
    
    if _CURRENT_TAPE:
        _CURRENT_TAPE.record_op("Sub", [a,b], out)
    return out        

def tf_pow(a,b):
    val = a.value**b.value
    out = Tensor(val)    
    
    if _CURRENT_TAPE:
        _CURRENT_TAPE.record_op("Pow", [a,b], out)
    return out          

def tf_truediv(a,b):
    val = a.value / b.value
    out = Tensor(val)    
    
    if _CURRENT_TAPE:
        _CURRENT_TAPE.record_op("Truediv", [a,b], out)
    return out 

# =============================================================================
# åå¾®åˆ†ä¸æ˜¯å°ã€Œè¡¨é”å¼ã€å¾®åˆ†ï¼Œ
# è€Œæ˜¯å°ã€Œå‡½æ•¸åœ¨åº§æ¨™æ–¹å‘ä¸Šçš„è®ŠåŒ–ç‡ã€åšå®šç¾©ã€‚
# =============================================================================
        
@register_gradient("MatMul")
def grad_matmul(grad, inputs):
    A, B = inputs
    grad_A = grad @ B.value.T
    grad_B = A.value.T @ grad   
    return grad_A, grad_B
        
@register_gradient("Add")
def grad_add(grad, inputs):
    # A, B = inputs
    return grad, grad
                
@register_gradient("Mul")
def grad_mul(grad, inputs):
    A, B = inputs
    grad_A = grad * B.value
    grad_B = grad * A.value 
    return grad_A, grad_B

@register_gradient("Sub")
def grad_sub(grad, inputs):
    # A, B = inputs
    return grad, -grad

@register_gradient("Pow")
def grad_pow(grad, inputs):
    A, B = inputs
    grad_A = grad * (B.value) * (A.value)**(B.value -1)
    grad_B = grad * np.exp(B.value*np.log(A.value))*np.log(A.value)
    return grad_A, grad_B

@register_gradient("Truediv")
def grad_truediv(grad, inputs):
    A, B = inputs
    grad_A = grad * 1/B.value
    grad_B = grad * -A.value *1/(B.value)**2
    return grad_A, grad_B
        
X = Tensor(np.random.normal(0,1,(10,3)),"X")
W = Tensor(np.random.normal(0,1,(3,3)),"W")
B = Tensor(np.random.normal(1.5,1,(10,3)),"B")

Y = Tensor(X.value @ ((np.array(range(9)).reshape(3,3) +1)/10) + (np.array(range(30)).reshape(10,3) +1)/10) 
Y.value
_CURRENT_TAPE = None

def loss_fcn(Y,Y_pred):
    result = tf_truediv(tf_pow(tf_sub(Y,Y_pred),
                               Tensor(2)),
                               Tensor(Y.value.shape[0]))
    return result

def optimize(grads,param,lambdas):
    para_update = []
    for g,p in zip(grads,param):
        p.value = p.value - lambdas*g
        para_update.append(p)
    return para_update
# =============================================================================
# äºŒã€ç‚ºä»€éº¼ä½ ç›´è¦ºæœƒè¦ºå¾—ã€Œåå¾®ä¹Ÿè©²å¾®åˆ° b è£¡çš„ aã€ï¼Ÿ
# å› ç‚ºä½ è…¦ä¸­åšçš„æ˜¯é€™ä»¶äº‹ï¼š
# ä½ å·²ç¶“æŠŠã€Œå¯¦éš›è·¯å¾‘ã€ä»£é€²å»äº†
# ä¸€æ—¦ä½ å¯«ï¼š
# f(a,b(a))
# ä½ å°±å·²ç¶“é€€å‡ºåå¾®çš„ä¸–ç•Œäº†ã€‚
# =============================================================================


epochs = 5000

for epoch in range(epochs):
    


    with Gradient_Tape() as tape:
        Y_pred = tf_add(tf_matmul(X,W),B)
        loss  = loss_fcn(Y,Y_pred)
        # record_op çµæŸ
    
    grads = tape.gradient(loss, [W,B])
    
    W,B = optimize(grads,[W,B],0.1)
    
    print(loss.value)








grad_W, grad_B = grads
print("\n--- Backward å®Œæˆ ---")

print("Gradient of W:\n", grad_W)
print("Gradient of B:\n", grad_B)

# =============================================================================
# # é©—è­‰å½¢ç‹€æ˜¯å¦æ­£ç¢º
# assert grad_X.shape == X.value.shape
# print("\nå½¢ç‹€æª¢æŸ¥é€šéï¼")
# =============================================================================
