# -*- coding: utf-8 -*-
"""
Created on Sun Dec 28 21:13:08 2025

@author: Ryan
"""


# =============================================================================
# MyClass()
# â†“
# PyObject_Call(MyClass)
# â†“
# PyType_Type.tp_call   â† å› ç‚º MyClass æ˜¯ type çš„ instance
# â†“
# type_call(type=MyClass)
# â†“
# MyClass.tp_new
# â†“
# MyClass.tp_init
# â†“
# instance
# =============================================================================
 
# =============================================================================
# ğŸ§© slot wrapper åˆ°åº•æ˜¯ä»€éº¼ï¼Ÿ
# ä¸æ˜¯ Python function
# ä¸æ˜¯ PyCFunction
# è€Œæ˜¯ã€ŒC å±¤ dispatcherã€
# 
# ç°¡åŒ–é•·ç›¸ï¼š
# 
# static PyObject *
# slot_tp_new(PyTypeObject *type, PyObject *args, PyObject *kwds)
# {
#     PyObject *meth = lookup("__new__", type);
#     return PyObject_Call(meth, args, kwds);
# }
# 
# 
# ğŸ‘‰ å®ƒçš„ä»»å‹™åªæœ‰ä¸€å€‹ï¼š
# 
# æŠŠ C slot å‘¼å«è½‰æˆ Python method å‘¼å«
# 
# ğŸ§© ç‚ºä»€éº¼ä¸èƒ½ç›´æ¥æŒ‡å‘ Python functionï¼Ÿ
# 
# å› ç‚ºï¼š
# 
# C slot ç°½åæ˜¯å›ºå®šçš„ï¼ˆABIï¼‰
# 
# Python function æ˜¯å‹•æ…‹ç‰©ä»¶
# 
# éœ€è¦ï¼š
# 
# argument unpack
# 
# descriptor ç¶å®š
# 
# MRO lookup
# 
# exception translation
# 
# ğŸ‘‰ slot wrapper æ˜¯å¿…è¦çš„ã€Œè½‰æ¥å±¤ã€
# =============================================================================

# =============================================================================
# ğŸ§  æœ€çµ‚ç¸½çµï¼ˆä½ ç¾åœ¨çš„ä½ç½®ï¼‰
# 
# ä½ ç¾åœ¨çš„ç†è§£å¯ä»¥æ¿ƒç¸®æˆé€™å¼µè¡¨ï¼š
# 
# æƒ…å¢ƒ	tp_repr æŒ‡å‘
# æ²’è¦†å¯« __repr__	C å¯¦ä½œï¼ˆbase.tp_reprï¼‰
# æœ‰è¦†å¯« __repr__	slot_tp_repr
# Python å‘¼å« obj.__repr__()	attribute lookup
# C / builtin å‘¼å« repr(obj)	tp_repr(obj)
# 
# ğŸ‘‰ slot wrapper ä¸æ˜¯å¤šæ­¤ä¸€èˆ‰
# ğŸ‘‰ å®ƒæ˜¯ç‚ºäº†è£œä¸Šã€ŒC slot å‘¼å«ä¸åš lookupã€é€™å€‹ç¼ºå£
# 
# =============================================================================


# watch æ±ºå®šçš„æ˜¯ã€Œæˆ‘è¦ä¸è¦çµæœã€
# ä¸æ˜¯ã€Œä½ èƒ½ä¸èƒ½åƒèˆ‡è¨ˆç®—ã€
# æ²’æœ‰ watch çš„è®Šæ•¸ä»ç„¶æœƒåƒèˆ‡ forward èˆ‡ backward çš„æ•¸å€¼è¨ˆç®—ï¼Œ
# åªæ˜¯å…¶æ¢¯åº¦ä¸æœƒè¢«å›å‚³ï¼Œä¹Ÿä¸æœƒè¢«ç”¨ä¾†æ›´æ–°åƒæ•¸ã€‚

import numpy as np
from collections import deque, defaultdict

_GRADIENT_REGISTRY = {}

def register_gradient(op_name):
    def wrapper(func):
        _GRADIENT_REGISTRY[op_name] = func
        return func #é‡é»åŒ…è£å™¨é‚„æ˜¯å›å‚³åŸæœ¬è‡ªå·±çš„å‡½æ•¸
    return wrapper


# å³ä½¿ OUT æ˜¯ Tensorï¼Œåªè¦å®ƒçš„ç¥–å…ˆæœ‰ Variableï¼Œå®ƒçš„ ID å°±æœƒè¢«è¨˜éŒ„ï¼Œæ¢¯åº¦å°±èƒ½å‚³å¾—å›å»ã€‚ é€™å°±æ˜¯ç‚ºä»€éº¼ä½ çš„ä»£ç¢¼èƒ½å‹•çš„åŸå› ï¼

class Tensor:
    def __init__(self,value, name=None):
        self.value = np.array(value, dtype=float)
        self.name = name
        self.id = id(self)
    def __repr__(self):
        return f"Tesnor(shape = {self.value.shape}, id={self.id})"

class Variable(Tensor):
    def __init__(self,value, name=None):
        super(Variable,self).__init__(value, name=None)
        
class Gradient_Tape:
    def __init__(self):
        self.ops = []
        self.watched_ids = set() # é€™è£¡ç´€éŒ„æ‰€æœ‰éœ€è¦è¿½è¹¤æ¢¯åº¦çš„ Tensor ID
        self.active = False
    def __enter__(self):
        self.active = True
        global _CURRENT_TAPE
        _CURRENT_TAPE = self 
        return self #ç•¶ä½ å¸Œæœ›åœ¨ with èªå¥ä¸­å–å¾—ä¸¦æ“ä½œé€™å€‹ä¸Šä¸‹æ–‡ç®¡ç†å™¨å¯¦ä¾‹æ™‚ã€‚
    def __exit__(self, exc_type, exc_val, exc_tb):
        self.active = False
        global _CURRENT_TAPE
        _CURRENT_TAPE = None
        # ç•¶é›¢é–‹ç•¶æ¬¡with Gradient_Tape() as tape å‘¼å« __exit__ _CURRENT_TAPEå°‡ä¸åœ¨è¨˜éŒ„é‹ç®—ä¸¦ä¸”å°‡å…¶æ·¨ç©º
    
    # âœ… æœ€æ¥è¿‘ TensorFlow eager tape çš„ç‰ˆæœ¬
    # forwardï¼šå…¨è¨˜
    def record_op(self, op_name, inputs, output):
        for x in inputs:
            if isinstance(x, Variable):
                self.watched_ids.add(x.id)
        self.ops.append({
            "op_name":op_name,
            "inputs":inputs,
            "output":output            
            })
        
    def watch(self,inputs):
        self.watched_ids.add(id(inputs))
    def gradient(self,target, sources):
        """
        å…¨éƒ¨é‹ç®—éƒ½åšå®Œ æ‰æœƒåœ¨å°loss fcnä½œå¾®åˆ† tape.gradient(Loss, [X, W, B]) 
        target : loss (Tensor) æœ€ä¸€é–‹å§‹é€²å…¥çµé»
        sources : list of params to update (list of Tensors)
        """
        # å“ªå€‹ output Tensor æ˜¯ç”±å“ªå€‹ op ç”¢ç”Ÿ
        producer_map = {}
        for op in self.ops:
            producer_map[op["output"].id] = op
            
        grad_counts = defaultdict(int)
        for op in self.ops:
            for x in op["inputs"]:
                grad_counts[x.id] += 1
        
        grads = defaultdict(int)
        grads[target.id] = np.ones_like(target.value)
        
        queue = deque([target])
        
        while queue:
            current_tensor = queue.popleft()
            current_grad = grads[current_tensor.id]
            
            if current_tensor.id not in producer_map:
                continue
            
            op_entry = producer_map[current_tensor.id]
            op_name = op_entry["op_name"]
            inputs = op_entry["inputs"]
            
            if op_name not in _GRADIENT_REGISTRY:
                raise ValueError(f"Op {op_name} æ²’æœ‰è¨»å†Šå¾®åˆ†å‡½æ•¸")
            grad_func = _GRADIENT_REGISTRY[op_name]  
            
            input_grads = grad_func(current_grad, inputs)
            
            if not isinstance(input_grads,(list, tuple)):
                input_grads = [input_grads]
            
            for x,g in zip(inputs, input_grads):
                # å»£æ’­ä¿®æ­£ (Broadcasting Fix)
                # =============================================================================
                # Broadcasting çš„æµç¨‹æ˜¯ï¼š
                # å…ˆåœ¨å·¦é‚Šè£œ 1ï¼Œä½¿ç¶­åº¦æ•¸ä¸€è‡´ï¼Œ
                # ç„¶å¾Œå¾æœ€å¾Œä¸€ç¶­é–‹å§‹é€ä¸€æ¯”å°ï¼Œ
                # è‹¥ä»»ä¸€ç¶­æ—¢ä¸ç›¸ç­‰ä¹Ÿä¸æ˜¯ 1ï¼Œå°±å ±éŒ¯ã€‚
                # æ¡†æ¶ä¸æœƒå…ˆæª¢æŸ¥æœ€å¾Œä¸€ç¶­ï¼Œä¹Ÿä¸æœƒæ¨æ–·èªæ„ã€‚
                # =============================================================================


                # =============================================================================
                # ç²¾æº–æè¿° broadcasting backward sum
                # å‡è¨­ forwardï¼š
                # A: (2,3,4)
                # B: (1,3,1)
                # C = A + B
                # broadcast ç¶­åº¦ = 0, 2
                # é broadcast ç¶­åº¦ = 1
                # backwardï¼š
                # æˆ‘å€‘è¦è¨ˆç®— dL/dBï¼Œshape = (1,3,1)
                # å° broadcast ç¶­åº¦ (0,2) åš sum
                # é broadcast ç¶­åº¦ (1) ä¿æŒå°æ‡‰ç´¢å¼•
                # =============================================================================

                if g.shape != x.value.shape:
                    while g.ndim > x.value.ndim:
                        g = g.sum(axis=0)
                    for i, dim in enumerate(x.value.shape):
                        if dim == 1 and g.shape[i] != 1:
                            g = g.sum(axis=i, keepdims=True)
                            
                grads[x.id] += g
                grad_counts[x.id] -= 1
                
                if grad_counts[x.id] == 0:
                    queue.append(x)
        return [ grads[s.id]  if s.id in self.watched_ids else None for s in sources]
            
def tf_matmul(a,b):
    val = a.value @ b.value
    out = Tensor(val)    
    
    if _CURRENT_TAPE:
        _CURRENT_TAPE.record_op("MatMul", [a,b], out)
    return out
        
def tf_add(a,b):
    val = a.value + b.value
    out = Tensor(val)    
    
    if _CURRENT_TAPE:
        _CURRENT_TAPE.record_op("Add", [a,b], out)
    return out

def tf_mul(a,b):
    val = a.value * b.value
    out = Tensor(val)    
    
    if _CURRENT_TAPE:
        _CURRENT_TAPE.record_op("Mul", [a,b], out)
    return out

def tf_sub(a,b):
    val = a.value - b.value
    out = Tensor(val)    
    
    if _CURRENT_TAPE:
        _CURRENT_TAPE.record_op("Sub", [a,b], out)
    return out        

def tf_pow(a,b):
    val = a.value**b.value
    out = Tensor(val)    
    
    if _CURRENT_TAPE:
        _CURRENT_TAPE.record_op("Pow", [a,b], out)
    return out          

def tf_truediv(a,b):
    val = a.value / b.value
    out = Tensor(val)    
    
    if _CURRENT_TAPE:
        _CURRENT_TAPE.record_op("Truediv", [a,b], out)
    return out 

def tf_reduce_sum(a):
    val = np.sum(a.value)
    out = Tensor(val)
    if _CURRENT_TAPE:
        _CURRENT_TAPE.record_op("ReduceSum", [a], out)
    return out

# =============================================================================
# åå¾®åˆ†ä¸æ˜¯å°ã€Œè¡¨é”å¼ã€å¾®åˆ†ï¼Œ
# è€Œæ˜¯å°ã€Œå‡½æ•¸åœ¨åº§æ¨™æ–¹å‘ä¸Šçš„è®ŠåŒ–ç‡ã€åšå®šç¾©ã€‚
# =============================================================================
        
@register_gradient("MatMul")
def grad_matmul(grad, inputs):
    A, B = inputs
    grad_A = grad @ B.value.T
    grad_B = A.value.T @ grad   
    return grad_A, grad_B
        
@register_gradient("Add")
def grad_add(grad, inputs):
    # A, B = inputs
    return grad, grad
                
@register_gradient("Mul")
def grad_mul(grad, inputs):
    A, B = inputs
    grad_A = grad * B.value
    grad_B = grad * A.value 
    return grad_A, grad_B

@register_gradient("Sub")
def grad_sub(grad, inputs):
    # A, B = inputs
    return grad, -grad

@register_gradient("Pow")
def grad_pow(grad, inputs):
    A, B = inputs
    grad_A = grad * (B.value) * (A.value)**(B.value -1)
    grad_B = grad * np.exp(B.value*np.log(A.value))*np.log(A.value)
    return grad_A, grad_B

@register_gradient("Truediv")
def grad_truediv(grad, inputs):
    A, B = inputs
    grad_A = grad * 1/B.value
    grad_B = grad * -A.value *1/(B.value)**2
    return grad_A, grad_B

@register_gradient("ReduceSum")
def grad_reduce_sum(grad, inputs):
    A, = inputs
    return grad * np.ones_like(A.value)
        
X = Tensor(np.random.normal(0,1,(10,3)),"X")
W = Variable(np.random.normal(0,1,(3,3)),"W")
B = Variable(np.random.normal(1.5,1,(1,3)),"B") # Parameter çš„ shape æ°¸é ä¸å« batch ç¶­åº¦
W.value

W_act = np.random.normal(0,1,(3,3))
B_act = np.random.normal(0,1,(1,3))
Y = Tensor(X.value @ W_act + B_act) 
Y.value
_CURRENT_TAPE = None

# =============================================================================
# def loss_fcn(Y,Y_pred):
#     result = tf_truediv(tf_pow(tf_sub(Y,Y_pred),
#                                Tensor(2)),
#                                Tensor(Y.value.shape[0]))
#     return result
# =============================================================================
def loss_fcn(Y, Y_pred):
    diff = tf_sub(Y, Y_pred)
    sq   = tf_pow(diff, Tensor(2))
    total = tf_reduce_sum(sq)
    batch_size = Tensor(np.array([Y.value.shape[0]]))
    mean  = tf_truediv(total,batch_size)
    return mean


def optimize(grads,param,lambdas):
    para_update = []
    for g,p in zip(grads,param):
        p.value = p.value - lambdas*g
        para_update.append(p)
    return para_update
# =============================================================================
# äºŒã€ç‚ºä»€éº¼ä½ ç›´è¦ºæœƒè¦ºå¾—ã€Œåå¾®ä¹Ÿè©²å¾®åˆ° b è£¡çš„ aã€ï¼Ÿ
# å› ç‚ºä½ è…¦ä¸­åšçš„æ˜¯é€™ä»¶äº‹ï¼š
# ä½ å·²ç¶“æŠŠã€Œå¯¦éš›è·¯å¾‘ã€ä»£é€²å»äº†
# ä¸€æ—¦ä½ å¯«ï¼š
# f(a,b(a))
# ä½ å°±å·²ç¶“é€€å‡ºåå¾®çš„ä¸–ç•Œäº†ã€‚
# =============================================================================


epochs = 500

for epoch in range(epochs):
    


    with Gradient_Tape() as tape:
        Y_pred = tf_add(tf_matmul(X,W),B)
        loss  = loss_fcn(Y,Y_pred)
        # record_op çµæŸ
    
    grads = tape.gradient(loss, [W,B])
    
    W,B = optimize(grads,[W,B],0.1)
    
    print(loss.value)


grad_W, grad_B = grads
print("\n--- Backward å®Œæˆ ---")
# è¨“ç·´åˆ°æœ€å¾Œæœ¬ä¾†æ¢¯åº¦å°±æ˜¯è¦é è¿‘0
print("Gradient of W:\n", grad_W) 
print("Gradient of B:\n", grad_B)
print("W_act:\n", W_act)
print("B_act:\n", B_act)
print("W:\n", W.value)
print("B:\n", B.value)

# =============================================================================
# # é©—è­‰å½¢ç‹€æ˜¯å¦æ­£ç¢º
# assert grad_X.shape == X.value.shape
# print("\nå½¢ç‹€æª¢æŸ¥é€šéï¼")
# =============================================================================
