# -*- coding: utf-8 -*-
"""
ç‚ºä»€éº¼ Optimizer çµ•å°ä¸èƒ½å¯«åœ¨ Tensor è£¡ï¼Ÿ
é€™ä¸æ˜¯ç¿’æ…£å•é¡Œï¼Œæ˜¯è²¬ä»»åˆ†é›¢ï¼ˆSeparation of Concernsï¼‰ã€‚
â‘  Tensor ä¸æ‡‰è©²çŸ¥é“ã€Œå¦‚ä½•è¢«æ›´æ–°ã€
Tensor çš„è²¬ä»»åªæœ‰ä¸‰ä»¶äº‹ï¼š
1ï¸âƒ£ å­˜ value
2ï¸âƒ£ å­˜ grad
3ï¸âƒ£ çŸ¥é“æ€éº¼æŠŠ grad å¾€å‰å‚³
"""


import numpy as np
from collections import deque, defaultdict
class Tensor:
    def __init__(self,value,inputs=[],grad_func=[],require_grad=True):
        self.value = np.array(value,dtype=float)
        self.inputs = inputs
        self.grad_func = grad_func
        self.grad = np.zeros_like(self.value)
        self.require_grad = require_grad
    def detach(self):
        return Tensor(self.value, requires_grad=False)    
    def backward(self, grad=None):
        if grad is None:
            grad = np.ones_like(self.value)
        self.grad =  grad  
        grad_counts = defaultdict(int)    
        
        node_to_visit = [self]
        visited_nodes = set()
        while node_to_visit:
            # é€™å€‹ while node_to_visit: æœƒçµæŸï¼Œç™¼ç”Ÿåœ¨ä½ å·²ç¶“æŠŠæ•´æ£µè¨ˆç®—åœ–ä¸€è·¯èµ°åˆ°æ‰€æœ‰ã€Œè‘‰ç¯€é»ï¼ˆinputs = []ï¼‰ã€ç‚ºæ­¢ï¼Œ
            current_node = node_to_visit.pop()
            if id(current_node) not in visited_nodes:
                visited_nodes.add(id(current_node))
            for i in current_node.inputs:
                grad_counts[id(i)] += 1
                node_to_visit.append(i)
        
        queue = deque([self])
        
        while queue:
            current_node = queue.popleft()
            
            if not current_node.inputs:
                continue
            
            for i,f in zip(current_node.inputs, current_node.grad_func):
                if not i.require_grad:
                    continue
                grad_input = f(current_node.grad) # éƒ½æ˜¯numpy arrayåšé‹ç®—
                # Backwardï¼ˆéˆå¼æ³•å‰‡ï¼‰æ˜¯åŠ ç¸½
# =============================================================================
#                 âˆ‚ğ‘¦/âˆ‚ğ‘¥=âˆ‚ğ‘“1/âˆ‚ğ‘¥+âˆ‚ğ‘“2/âˆ‚ğ‘¥
#             	â€‹  x
#                  / \
#                f1   f2
#                  \ /
#                   y = f1(x) + f2(x)
# =============================================================================

                if grad_input.shape != i.grad.shape:
                     while grad_input.ndim > i.grad.ndim:
                        grad_input = grad_input.sum(axis=0) # autograd åªè² è²¬ã€Œæ•¸å­¸æ­£ç¢ºçš„å°æ•¸ã€
                     for ii, dim in enumerate(i.grad.shape):
                        if dim == 1 and grad_input.shape[i] != 1:
                            grad_input = grad_input.sum(axis=ii, keepdims=True)
                
                
                
                i.grad += grad_input
                grad_counts[id(i)] -= 1
                if grad_counts[id(i)] == 0:
                    queue.append(i)
        
        
    def __matmul__(self, other):
        if not isinstance(other, Tensor):
            other = Tensor(other)
        out = self.value @ other.value
        
        return Tensor(out,inputs=[self,other],grad_func=[lambda g:g @ other.value.T,lambda g:self.value.T @ g])
    def __add__(self, other):
        if not isinstance(other, Tensor): 
            other = Tensor(other)
        return Tensor(self.value + other.value, inputs=[self, other], grad_func=[lambda g: g, lambda g: g])
    def __radd__(self, other): 
        return self.__add__(other)
    def __mul__(self, other):
        if not isinstance(other, Tensor): 
            other = Tensor(other)
        return Tensor(self.value * other.value, inputs=[self, other], grad_func=[lambda g: g * other.value, lambda g: g * self.value])
    def __rmul__(self, other): 
        return self.__mul__(other)
    
    
# æ¸¬è©¦
X = Tensor(np.random.normal(0,1,(3,3)), require_grad=False)     
B = Tensor(np.random.normal(0,1,(3,3)))      

# è¤‡é›œä¸€é»çš„åœ–ï¼šX è¢«ç”¨äº†å…©æ¬¡ (Diamond Pattern)
Y = X @ X + 3 * X + B
Y.value
Y.backward()

print("åŸ·è¡ŒæˆåŠŸï¼Œæ²’æœ‰éè¿´")
print("X çš„æ¢¯åº¦:\n", B.grad)    
    
    
    
    
    